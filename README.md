This is the code for the work described in the paper "Attuning Ontology Alignments to Semantically Heterogeneous Multi-Agent Interactions", presented at ECAI 2016.

The following files are included:

- interaction.py: contains the basic classes necessary for the interaction, such as Interaction Model and Alignment

- generator.py: contains the random generator for all the classes in the file above. In particular, contains a random protocol generator and a method that creates alignments with particular values of precision and recall with respect to another one.

- issa.py: contains the definition of the four agents that are described in the paper. These are AgentWA, that only uses an alignment; ISSAgent, that only learns an alignment from the experience; ISSAgentWA, a straightforward combination between the previous two; AgentWAExp, a more sophisticated combination

- examples.py: contains the toy examples that I used for preliminary experimentation, in particular an alignment between european countries and its capitals, and the Travel Agency scenario presented in the paper

- experiments.py: the experimentation that is presented in the paper

- example.py: a demo showing the behaviour of agents. Explained now in detail.


########### The EXAMPLE ##########

The example shows the evolution of the alignment learning by two agents that use an interaction model with 50 states.
The two agents are created with different protocols and alignment, and by interacting with each other they learn the alignment between its vocabularies. I included two examples:
- 'europe': agents use a vocabulary that consists on european countries and capitals, and protocols are generated randomly.
- 'travel': agents use the travel agency scenario presented in the paper

When agents have previous alignments, they have precision and recall 0.5 with respect to the correct alignment.


To execute the example, call 

 --------------   python example.py [-a -i -p -v -s] --------------

It accepts 5 parameters:

* -a  determines the kind of agent that will be used. The possibilities are:
	* 0: AgentWA, an agent with an alignment but without learning
	* 1: ISSAgent, an agent with learning but without any alignment
	* 2: ISSAgentWA, an agent with learning and alignment, and a simple technique to combine both
	* 3: AgentWAExp, an agent with learning and alignment and a more involved technique to combine both
	Defaults to 3.

* -i determines the quantity of interactions to be performed by the agents. Defaults to 100

* -p determines the example to show. Can be 'europe' or 'travel' and defaults to 'europe'

* -v determines the verbosity. If  it is 1, agents show the dynamics of the interaction and their alignments in each step. Defaults to 0.

* -s determines the size of the protocol to be used, if generating it randomly. Defaults to 90. 

The demo in no verbose mode will print the result of each interaction, as well as the final percentage of successful ones.
